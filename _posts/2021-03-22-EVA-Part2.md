---
layout: post
title:  EVA AI Seminar - Part 2
date: 2021-03-22
modified: 2021-03-23
categories: english MSc homework EVA-AI-seminar
comments: true
summary: This is the second post related to the course "EVA Artificial Intelligence Seminar". In this post, I review the paper "Attention is All You Need" by Vaswani et al. which I have chosen as my seminar reading.
---

*This blogpost was created as a homework assignment in the Artificial Intelligence seminar. In this assignment we have to write a blogpost in which we evaluate a paper on structure and writing style.*

# Attention Is All You Need
I chose the Paper "Attention Is All You Need" [1] as my seminar literature. This paper was written by members of Google
Brain and Google Research and was presented at the 31st Conference on Neural Information Processing Systems (NIPS) in 2017.
It introduces a new architecture called "Transformer". Personally I believe this architecture has the potential to sustainable change the field of deep learning.
Although it was originally developed for NLP, the Transformer network recently finds its way into other fields such as ASR [2] and
computer vision [3]. For more information on why I chose this paper, feel free to <a href="/assets/downloads/2021-03-01-AI_Seminar_Thesis_Proposal.pdf" target="_blank">download my thesis proposal</a>.

## Target Audience
Since the paper was presented at NIPS, the target audience are specialists in the field of AI. This is also reflected in the writing style. 
Readers must have a certain knowledge about deep learning. Especially the abstract and the conclusion emphasize the performance. To understand this sections, 
metrics like BELU score must be known. The authors used these metrics to explain the improvement compared to the previous state-of-the-art.

## Title
Personally, I don't like the title "Attention Is All You Need". The title intends more to attract readers than to tell anything about the content.
Only the term "Attention" refers to the attention layers used in the proposed Transformer architecture. However, this only becomes obvious after reading the abstract.
Personally, I prefer more descriptive titles like "Transformer: A New Architecture Solely Based on Attention Mechanisms". 
But this title probably generates less Attention.

## Abstract
In this paper, a new architecture with remarkable performance is introduced. The authors particularly emphasize the performance and the training speed in the abstract.
The performance is clearly explained with metrics. However, the training speed should be described in more detail. 
The authors only wrote: "... training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature". 
They do not mention the type of GPU used nor describe which model from the literature is meant.

## Conclusion
In the conclusion, not only are the results well summarised, but also ideas for future research are presented. For example, the 
authors wrote "... extend the Transformer to [...] handle large inputs and outputs such as images, audio and video".
Today we know that the authors were right and that transformers are used in various areas of deep learning [2]-[4].
I always find such ideas very helpful, as the authors often have a good intuition about their work. In this way, they can help other researchers to pursue related research topics.

## Introduction and Background
What I really like about this paper are these two sections. Personally, I have a background in computer vision, and only know the basics of NLP. 
These two sections explain the recent progress in NLP and describe how attention layers have led to performance improvements. 
In doing so, they also motivate why their architecture is solely based on attention mechanisms.
In this way, the authors explain not only what they have developed but also why. This allows to understand their decisions. Furthermore, this explanation can also be very helpful for the development of other architectures.

## Main Part
The main part of the paper consists of four sections:

- **Model Architecture**: The architecture of the Transformer network
- **Why Self-Attention**: Comparison between self-attention layers, recurrent layers and convolutional layers
- **Training**: How the network was trained
- **Results**: Results in different tasks such as machine translation or constituency parsing

The model architecture is described at a good level of abstraction. The best practices used are briefly explained and appropriate references are added. On the other hand, the major innovations are explained in more detail, which is very helpful.
They also used appropriate graphics to describe the architecture. The graphics are mostly self-explanatory, but the captions should be a bit more detailed.
I think it makes a lot of sense that they introduced a separate chapter for attention mechanisms afterwards. In this chapter, the main differences to existing architectures are not only described, but also motivated.

Personally, I find the training and results sections less interesting, but they are certainly relevant for a new architecture. For me, these sections are too detailed and part of them could have been moved to the appendix.
I would prefer a more compact overview instead. If someone wants to reproduce the results, they could still study the appendix.

## Style, Wording, Layout
The paper is written in a very good style. The sections are well structured and not too long. Also, the most important findings are explained in detail and appropriate references are given for proven methods. Overall, the paper is very pleasant to read, even if you aren't an expert in the field of NLP.

# Why it is a good paper
The title and the abstract did not appeal to me much at the beginning. The title says very little about the content and the abstract mainly argues the relevance with metrics which I could only poorly interpret. But after that follows a very well written and interesting content.
The authors have found a very good mix to describe the state-of-the-art and how their network differs from it. They motivate their decisions and show the advantages. Moreover, the paper is very pleasantly written and easy to understand. This is very remarkable, as it describes rather a complex architecture.
I can only recommend this paper, especially to people who are interested in deep learning.


- [1] Vaswani et al., "Attention Is All You Need", 2017, 31st Conference on Neural Information Processing Systems (NIPS)
- [2] Miao et al., "Transformer-based online ctc/attention end-to-end speech recognition architecture", 2020, IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)
- [3] Dosovitskiy et al., "An image is worth 16x16 words: Transformers for image recognition at scale", 2020
- [4] Parmar et al., "Image Transformer", 2018, International Conference on Machine Learning

