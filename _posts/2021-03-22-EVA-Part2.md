---
layout: post
title:  EVA AI Seminar - Part 2
date: 2021-03-22
modified: 2021-03-22
categories: english MSc homework EVA-AI-seminar
comments: true
summary: This is the second post related to the course "EVA Artificial Intelligence Seminar". In this post, I review the paper "Attention is All You Need" by Vaswani et al. which I have chosen as my seminar reading.
---

*This blogpost was created as a homework assignment in the Artificial Intelligence seminar. In this assignment we have to write a blogpost in which we evaluate a selected paper on structure and writing style, but not on content.*

# Attention Is All You Need
I chose the Paper "Attention Is All You Need" [1] as my seminar literature. This paper was written by members of Google
Brain and Google Research. It was presented at the 31st Conference on Neural Information Processing Systems (NIPS) in 2017.
It introduced a new architecture called "Transformer". Personally I believe it has the potential to sustainable change the field of deep learning.
Although this architecture was originally developed for NLP, it recently finds its way into other fields such as ASR [2] and
computer vision [3]. For more information on why I chose this paper, feel free to [download my thesis proposal](/assets/downloads/2021-03-01-AI_Seminar_Thesis_Proposal.pdf).

## Target Audience
Since the paper was presented at NIPS, the target audience are specialists in the field of AI. This is also reflected in the writing style. 
Readers must have a certain knowledge in deep learning. Especially in the abstract and conclusion, performance is emphasized and 
metrics like BELU score should be known to fully understand the improvements compared to the previous state-of-the-art.
In the other sections are mostly references added, that explain the used terms and concepts.

## Title
Personally, I don't like the title "Attention Is All You Need." It's definitely a "catchy" title to attract readers. 
However, it says very little about the content. Only the term "Attention" refers to the attention layers used in the proposed Transformer architecture. 
Personally, I prefer more descriptive titles like "Transformer: A New Architecture Solely Based on Attention Mechanisms". 
But this title probably generates less Attention.

## Abstract
In this paper, a new architecture with remarkable performance is presented. The authors particularly emphasize the performance and the training speed in the abstract.
The performance is clearly supported with metrics in the abstract. However, the training speed should be described in more detail. 
The authors only write "... training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature". 
They do not mention the type of GPU and the training costs of the best models from the literature.

## Conclusion
In the conclusion, not only are the results well summarised, but also ideas for future research are presented. For example, the 
authors wrote "... extend the Transformer to [...] handle large inputs and outputs such as images, audio and video".
Today we know that the authors were right and that transformers are used used in various areas of deep learning [2]-[4].

## Introduction and Background
What I really like about this paper are these two sections. Personally, I have a background in computer vision, but I know the basic of NLP. 
These two sections explain the evolution of NLP and describe how attention layers have led to performance improvements. 
In doing so, they also motivate why their architecture is solely based only on attention mechanisms.
They also refer to many fundamental papers so that less experienced readers have the opportunity to become familiar with the relevant literature.

## Main Part
The main part of the paper consists of four sections:

- **Model Architecture**: The architecture of the Transformer network
- **Why Self-Attention**: Comparison between self-attention layers, recurrent layers and convolutional layers
- **Training**: How the network was trained
- **Results**: Results in different tasks such as machine translation or constituency parsing

The model architecture is described at a good level of abstraction. They also use appropriate graphics. The graphics are mostly self-explanatory, but the captions should be a bit more detailed.
I think it makes a lot of sense that they introduced a separate chapter for attention mechanisms afterwards. In this chapter, the main differences to existing architectures are described and motivated.

Personally, I find the Training and Results sections less interesting, but they are certainly relevant for a new architecture. For me, these sections are too detailed and part of them could have been moved to the appendix.

## Style, Wording, Layout
The paper is written in a very good style. The sections are well structured and not too long. Also, the most important findings are explained in detail and appropriate references are given for proven methods. Overall, the paper is very pleasant to read, even if you aren't an expert in the field of NLP.

# Why it is a good paper
The title and the abstract did not appeal to me much at the beginning. The title says very little about the content and the abstract mainly argues the relevance with metrics which I could only poorly interpret. But after that follows a very well written and interesting content.
The authors have found a very good mix to describe the state-of-the-art and how their network differs from it. They motivate their decisions and show the advantages. Moreover, the paper is very pleasantly written and easy to understand. This is very remarkable, as it describes rather a complex architecture.
I can only recommend this paper, especially to people who are interested in Deep Learning.


- [1] Vaswani et al., "Attention Is All You Need", 2017, 31st Conference on Neural Information Processing Systems (NIPS)
- [2] Miao et al., "Transformer-based online ctc/attention end-to-end speech recognition architecture", 2020, IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)
- [3] Dosovitskiy et al., "An image is worth 16x16 words: Transformers for image recognition at scale", 2020
- [4] Parmar et al., "Image Transformer", 2018, International Conference on Machine Learning

