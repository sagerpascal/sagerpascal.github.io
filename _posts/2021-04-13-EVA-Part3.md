---
layout: post
title:  EVA AI Seminar - Part 3 - Transformer Network
date: 2021-04-13
modified: 2021-04-14
categories: english MSc homework EVA-AI-seminar
comments: true
summary: This is the third post related to the course "EVA Artificial Intelligence Seminar". In this post, I describe what the Transformer network is, how it works and what its novelty is.
---

*This blogpost was created as a homework assignment in the [Artificial Intelligence seminar](https://moodle.msengineering.ch/mod/data/view.php?d=62&rid=3128&filter=1). In this seminar we have to write a series of blogposts. In this post, I describe how Transformers work.*

# Transformer Network
I chose the Paper "Attention Is All You Need" [1] as my seminar literature. This paper was written by Vaswani et al. (Google
Brain and Google Research) and was presented at the 31st Conference on Neural Information Processing Systems (NIPS) in 2017.
It introduces a new architecture called "Transformer".

Transformers are designed to handle sequential data. Before this novel architecture was developed, mainly reccurent neural networks (RNNs) such as long short-term memory (LSTM) or gated recurrent unit (GRU) were used to handle this kind of data. 
The main difference between RNNs and Transformer is that Transformers do not require the sequential data to be processed in the sequential order. For example, the end of an audio file
can be processed before the beginning of the file. This allows Transformers to use much more parallelization than RNNs. Therefore, they can be trained more efficient, which
enables training on larger datasets. This has led to the development of huge systems such as GPT-3 [2] or BERT [3] with 100+ million parameters and the improvement of the state-of-the-art in various sequence-to-sequence tasks.

## Background
For more than two decades, RNNs have been firmly established as state-of-the-art approaches in sequence modeling. These recurrent models have a "memory" in the sense that they
calculate a hidden state $$h_t$$ based on a previous hidden state $$h_{t-1}$$ and the input at time $$t$$. Since the hidden state $$h_t$$ is also
used to calculate the next hidden state $$h_{t+1}$$, the network can keep a part of the hidden state and therefore memorize it. The disadvantage is
that the hidden states as well as the inputs must be processed sequentially. This precludes parallelization within training examples.

Besides RNNs, also attention mechanisms were quite popular. An attention mechanism predicts for a given symbol of an input sequence which other symbols are particularly interesting and should therefore receive more "attention".
However, before the development of the Transformer network, these attention mechanism were always used in conjunction with RNNs. The novelty of Transformer networks is that they rely entirely on attention mechanisms.
They highlight the fact that the attention mechanisms alone are powerful enough to achieve the performance of RNNs with attention. This means that Transformers don't use a recurrent sequential processing.
Therefore, they can use significantly more parallelization which leads faster training and the usage of more data.

## Network Architecture
The model consists of a typical encoder-decoder architecture as shown in figure 1. The input signal is first converted in a vector, i.e. an input embedding.
One of the main advantages of the Transformer network is the parallelization which means the whole input sequence is fed into the network at once.
Therefore, we have to add some information about the position of the single symbols. Otherwise the network
would for example for the phrase "Hello World" not know that "Hello" comes before "World". This information about the positioning is added to the embedding vector using positional encoding.
A positional encoding can be done using a combination of sinus and cosinus signals, which output a unique encoding for each time-step $$t$$.

<p align="center">
    <img src="/assets/images/posts/2021-04-13-EVA-Part3/transformer_architecture.PNG" alt="Transformer Architecture" width="60%" />
    <br>
    <i> Figure 1: The Transformer architecture (source: [1]).</i>
</p>

The input with the positional encoding is then fed into the encoder. Each layer in the encoder processes its input to generate encodings, 
containing information about which parts of the inputs are relevant to each other. This is done using an attention mechanism.
Multiple encoder blocks can be combined - then each encoder block passes its set of encodings to the next block.

The decoder has the same number of blocks and does the opposite operation: Each layer takes all the encodings to generate an output sequence.
In comparison to the encoder, a decoder block consists of two attention mechanisms. The first attention mechanism uses the outputs of previous decoder block, 
the second one takes the encodings from the corresponding encoder block.

Now the question is, what is fed into the first encoder block. This is somewhat unusual for classical neural networks: It is the target sequence! 
However, this target sequence is shifted to the right and masked. This way the decoder does not see the target when he wants to predict a specific symbol.

Let's assume the symbols $$[S_{t-2}, S_{t-1}, S_{t} ]$$ are given and the symbols $$[S_{t+1}, S_{t+2}]$$ shall be predicted. In this case, the sequence $$[S_{t-2}, S_{t-1}, S_{t}]$$
is fed into the encoder and the sequence $$[S_{t}, S_{t+1}]$$ is fed into the decoder. The Transformer then predicts the first symbol $$S_{t+1}$$. In order that the decoder not already
has the correct target symbol as input, part of this sequence is masked. In this case, $$S_{t+1}$$ gets masked and the sequence $$[S_{t}, ?]$$ is fed into the attention mechanism.
After predicting the symbol $$S_{t+1}$$, the masking at the input of the decoder gets removed and the sequence $$[S_{t}, S_{t+1}]$$ is fed into the attention layer. The network
then predicts the second symbol $$S_{t+2}$$. 

Since this explanation is a rather abstract one, I try to explain the functionality in the following chapter with an example.

### An Explanatory Example
A typical task of a sequence-to-sequence models is machine translation. Imagine we want to translate the English sentence *"The AI seminar is great"* into German using a Transformer network with two encoders and two decoder blocks. During training time, both sentences are given. The English sentence represents the known data (input into the encoder) and the German sentence represents the data to be predicted (output of the decoder). First, we encode both sentences into symbols, i.e. feature vectors. Then we enrich these symbols with positional encoding so that the network knows that *"The"* was the first word, *"AI"* the second and so on. We do this for both sentences, the German and the English one. The English sentence is then fed into the encoder.

The attention mechanisms are used to calculate the attention of all words, given a specific word. For example, the word *"seminar"* will get a lot of attention for the word *"The"*, because depending on the noun (i.e. *"seminar"*), the German translation of *"The"* is different (i.e. *"der"*, *"die"* or *"das"*). Since the encoder has two blocks, this is done twice.

Afterwards, the encoded representations from the encoder are fed into the decoder. Based on the encoded representation, the decoder then decides which word would be a proper translation. A good German translation for the whole sentence is *"Das AI Seminar ist super"*. Therefore, the encoder puts a lot of attention on *"The"* (since this word has to be translated) and probably some attention on *"seminar"* (since this word influences the translation) for the first word *(Note: This is an intuitive explanation and not a proven theory - The attention can be different in a practical application)*.

The translation is done by the decoder using two attention mechanism. One of them directly processes the encodings from the encoder, the other one the correct German translation shifted to the right. The German translation is shifted by 1, in order that the decoder not already sees the solution. This means, that if the first word is translated (i.e. *"The"*) the input in the decoder is *"None"*. Then the decoder should translate *"The"* to *"Das"*, but of course he could get it wrong and translate it to *"Der"*. After he has translated the first word, the decoder sees the correct translation (i.e. *"Das"*) and translates the next word. By doing so, the decoder can always rely on the correct translation of the previous part of sentence and has therefore a stable training.

At inference time, the translation is done in the same way - but of course the German sentence is not known anymore. The network then just translates one word. This word is then fed into the decoder in a reccurent manner. In the next loop the first two words get translated. This means that during inference time the words are processed sequentially.



## Application of Transformers

The paper on Transformer was published about 3.5 years ago. In the conclusion, the authors suggested that this type of network could also be used in other areas such as computer vision or audio processing (e.g. speaker recognition and speech recognition). This is reason enough to list a few exciting applications of Transformer networks besides the typical NLP tasks like machine translation, document summarisation or document generation. For example, Transformers were successfully applied in...

- Image Classification [4]
- Image Generation [5]
- Protein Prediction [6]
- Speech Recognition [7]
- Audio to Text Translation [8]
- and many more applications...

If you want to learn more about these applications, I recommend reading the referenced papers. They are worth it ;)


## Conclusion
In this blogpost, I rather tried to provide an intuition about the architecture than to explain everything in great detail. If you want to learn more about Transformer, especially how the self-attention layers works and what their advantages are, I can only recommend reading the paper "Attention Is All You Need" [1]. This paper explains the architecture on a good level of abstraction. It is written in a very clear and understandable way, despite the high complexity of the architecture. For the implementation I recommend using libraries, for example [PyTorch Transformer](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html)  or [Hugging Face](https://huggingface.co). But be careful, the devil is often in the details like the correct masking and shifting of the decoder input sequence.


- [1] Vaswani et al., "Attention Is All You Need", 2017, 31st Conference on Neural Information Processing Systems (NIPS)
- [2] Brown et al., "Language Models are Few-Shot Learners", 2020
- [3] Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", 2018
- [4] Dosovitskiy et al, "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale", 2021, ICLR
- [5] Parmar et al., "Image Transformer", 2018, PMLR
- [6] Namibar et al., "Transforming the Language of Life: Transformer Neural Networks for Protein Prediction Tasks", 2020
- [7] Chang et al. "End-to-End Multi-speaker Speech Recognition with Transformer", 2020
- [8] Baevski et al., "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations", 2020

