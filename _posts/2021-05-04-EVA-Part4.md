---
layout: post
title:  EVA AI Seminar - Part 4 - Self-Evaluation
date: 2021-05-04
modified: 2021-05-04
categories: english MSc homework EVA-AI-seminar
comments: true
summary: This is the 5th post related to the course "EVA Artificial Intelligence Seminar". In this post, I provide a scientific review of the paper "Attention Is All You Need".
---

*This blogpost was created as a homework assignment in the [Artificial Intelligence seminar](https://moodle.msengineering.ch/mod/data/view.php?d=62&rid=3128&filter=1). In this seminar we have to write a series of blogposts. In this post, I provide a scientific review of the paper "Attention Is All You Need".*

# About the Paper
I chose the Paper "Attention Is All You Need" [1] as my seminar literature. This paper was written by Vaswani et al. (Google
Brain and Google Research) and was presented at the 31st Conference on Neural Information Processing Systems (NeuIPS) in 2017.
It introduces a new architecture called "Transformer".

Transformer can be used for sequence-to-sequence modeling. The novelty of this architecture is that it is soley based on attention mechanisms.
These mechanims can be computed very efficient and the system has therefore a reduced training time. Transformer achieve state-of-the-art results on translation, and significantly improves the performance of seq2seq only parsing models.

# Review
The following is my personal review of the paper "Attention Is All You Need". I follow the IJCAI review template and rate the paper on the basis of 6 criteria with 1-10 stars. In addition, I comment briefly on each of the criteria and provide an overall score with comments at the end. 

###### Relevance

**Rating: 9/10**

The paper was presented at the 31st Conference on Neural Information Processing Systems (NeuIPS) in 2017. Therefore, the
relevance for this conference is assessed. NeuIPS is an interdisciplinary conference that "invites submissions presenting new and original research on topics including but not limited to Machine Learning, Deep Learning, Applications (e.g. speech recognition and NLP), [...]"

The paper presents a new architecture that can be used for sequence-to-sequence modelling. It is a deep learning architecture that can be applied to audio processing and NLP. Besides the much better training efficiency, Transformers have achieved new state-of-the-art results for translation. Therefore, this paper is considered very relevant, as it (a) corresponds to the content of the conference and (b) is expected to be of interest to researchers in the field.


###### Significance

**Rating: 9/10**

The paper is probably one of the most significant of recent years. Not only has it been cited almost 15,000 times in the last four years, it has also opened up completely new areas of research. 
The architecture was originally used for NLP, where it improved performance for automatic translation. Since then, Transformers have been used for many state-of-the-art models such as GPT v1-v3, BERT, RoBERTa, XLM, ... ... Besides NLP, transformers are now also used in audio processing, computer vision (Vision Transformer), time series predictions or reinfrcment learning.

The architecture itself is also constantly evolving and has led to the so-called X-formers. X-former is an umbrella term for various further developments such as Reformer, Linformer, Performer, Longformer, etc.

###### Originality

**Rating: 8/10**
:
Before the development of Transformer, attention mechanisms were only used in combination with RNNs. The basic idea of Transformer is that the attention mechanisms alone are powerful enough to achieve the performance of RNNs with attention.
This idea is fundamentally new and leads to the development of more efficient architectures.


###### Technical Quality

**Rating: 5/10**

The architecture is well justified and described. Experiments were then carried out to show the performance.
However, the paper lacks a detailed appendix. Since the appendix has been omitted, some implementation details are included in the "Results" section. This makes the results tedious to read as they contain some architectural notes. It would have been more appropriate to move this part to the appendix and describe it in more detail there. In the current form, the results are difficult to reproduce.

###### Clarity and Quality of Writing

**Rating: 7/10**

The paper is written in a very good style. The ideas and the architecture are easy to understand despite the high complexity. Appropriate references and figures have been included. Although the figures are mostly self-explanatory, the captions should be more detailed. For example, if the figure for the attention heads is viewed without reading the text, the functionality is not fully comprehensible.

###### Scholarship, i.e. Scientific Context

**Rating: 6/10**

The paper describes relevant work and cites it appropriately. Only the statement that the state-of-the-art was exceeded with a fraction of the training time is too little supported. Here, a precise specification of what the state-of-the-art is and how long they were trained would be appropriate.

###### Overall Score

**Rating: 8/10**

Overall, this is a very good paper and a clear accept. Nevertheless, some things should be better described:

- Inference with Transformer is not described - should be discussed more
- The appendix should contain more details about the training, e.g. used hyperparameters
- The paper has a lot of content and misses sometimes in-depth explanations

###### Confidence on my Assessment

**Rating: 6/10**

I don't have complete knowledge of the area, but can assess the value of the work. Especially, I have already implemented
a Transformer network but in another context than NLP.

# Conclusion
Overall, this is a very good paper with only a few shortcomings. The paper is very influential and has significantly influenced Deep Learning.



- [1] Vaswani et al., "Attention Is All You Need", 2017, 31st Conference on Neural Information Processing Systems (NIPS)




